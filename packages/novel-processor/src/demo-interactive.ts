/**
 * å°è¯´å¤„ç†å™¨ - äº¤äº’å¼ Demo
 *
 * æ¯ä¸€æ­¥éƒ½ä¼šè¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­ï¼Œé€‚åˆé€æ­¥è°ƒè¯•å’Œè§‚å¯Ÿæ•ˆæœ
 *
 * ä½¿ç”¨æ–¹æ³•ï¼š
 * 1. è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEY / OPENAI_API_KEY / DEEPSEEK_API_KEY
 * 2. è¿è¡Œï¼šbun run src/demo-interactive.ts
 */

import "dotenv/config"
import { readFile, writeFile } from "node:fs/promises"
import { join } from "node:path"
import { createInterface } from "node:readline"
import { TokenChunker } from "./libs/chunker"
import { LLMClient } from "./libs/llm"
import { logger } from "./libs/logger"
import { mergeAnalyses, NovelAnalyzer } from "./modules/analyzer"
import { CharacterDesigner } from "./modules/designer"
import { NovelRewriter } from "./modules/rewriter"
import type { Character, ChunkAnalysis, Episode } from "./types"

// ============ ç±»å‹å®šä¹‰ ============
interface MergedPlotGraph {
  fullSummary: string
  allEvents: import("./types").Event[]
  allCharacters: Map<string, Character>
  allRelations: import("./types").CausalRelation[]
}

// ============ é…ç½® ============
const NOVEL_PATH = join(__dirname, "../assets/7421252097296829502_small.txt")
const timestamp = new Date().toISOString().replace(/[:.]/g, "-").slice(0, 16)
const OUTPUT_DIR = join(__dirname, "../output", timestamp)

// ============ äº¤äº’å·¥å…· ============

const rl = createInterface({
  input: process.stdin,
  output: process.stdout,
})

/**
 * è¯¢é—®ç”¨æˆ·æ˜¯å¦ç»§ç»­
 */
async function askContinue(stepName: string, details?: string): Promise<boolean> {
  console.log("")
  logger.info(`â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`)
  logger.info(`ğŸ¤” å³å°†æ‰§è¡Œ: ${stepName}`)
  if (details) {
    logger.info(details)
  }
  logger.info(`â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”`)

  const answer = await new Promise<string>((resolve) => {
    rl.question("â¤ æ˜¯å¦ç»§ç»­? [Y/n/skip=è·³è¿‡æ­¤æ­¥/q=é€€å‡º]: ", resolve)
  })

  const normalized = answer.trim().toLowerCase()

  if (normalized === "q" || normalized === "quit" || normalized === "exit") {
    logger.info("ğŸ‘‹ ç”¨æˆ·é€‰æ‹©é€€å‡º")
    process.exit(0)
  }

  if (normalized === "skip" || normalized === "s") {
    return false // è·³è¿‡æ­¤æ­¥éª¤
  }

  return normalized === "" || normalized === "y" || normalized === "yes"
}

/**
 * è¯¢é—®ç”¨æˆ·è¾“å…¥æ•°å­—
 */
async function askNumber(prompt: string, defaultValue: number): Promise<number> {
  const answer = await new Promise<string>((resolve) => {
    rl.question(`${prompt} [é»˜è®¤: ${defaultValue}]: `, resolve)
  })
  const num = parseInt(answer.trim(), 10)
  return isNaN(num) ? defaultValue : num
}

// ============ ä¸»æµç¨‹ ============

async function main() {
  logger.info("ğŸ“š å°è¯´å¤„ç†å™¨ - äº¤äº’å¼ Demo (é“¾è·¯ä¼˜åŒ–ç‰ˆ)")
  logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”")
  logger.info("æ¯ä¸€æ­¥æ‚¨å¯ä»¥é€‰æ‹©ï¼šç»§ç»­(Y)ã€è·³è¿‡(skip)ã€é€€å‡º(q)")
  logger.info("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")

  // ===== æ­¥éª¤ 0: æ£€æŸ¥ç¯å¢ƒ =====
  const hasApiKey =
    process.env.ARK_API_KEY || process.env.OPENAI_API_KEY || process.env.DEEPSEEK_API_KEY
  if (!hasApiKey) {
    logger.warn("âš ï¸  æœªæ£€æµ‹åˆ° LLM API Key")
    process.exit(1)
  }

  // ===== æ­¥éª¤ 1: è¯»å–å°è¯´ =====
  const novelText = await readFile(NOVEL_PATH, "utf-8")
  logger.info(`   âœ… å­—ç¬¦æ•°: ${novelText.length.toLocaleString()}`)

  // ===== æ­¥éª¤ 2: æ–‡æœ¬ç»Ÿè®¡ =====
  const chunker = new TokenChunker(8000, 500)
  const stats = chunker.getStats(novelText)
  logger.info(`   ğŸ“Š é¢„è®¡åˆ‡å—æ•°: ${stats.estimatedChunks}`)

  // ===== æ­¥éª¤ 3: åˆ‡å— =====
  const chunks = chunker.chunk(novelText)
  logger.info(`   âœ… å®é™…åˆ‡åˆ†æˆ ${chunks.length} å—`)

  const maxChunks = await askNumber(
    `\nğŸ“‹ å…±æœ‰ ${chunks.length} ä¸ªå—ï¼Œè¦åˆ†æå¤šå°‘ä¸ªå—?`,
    Math.min(3, chunks.length)
  )
  const chunksToAnalyze = chunks.slice(0, maxChunks)

  // ===== æ­¥éª¤ 4: LLM æ·±åº¦åˆ†æ (å·²èåˆè§’è‰²å‘ç°) =====
  const llm = new LLMClient()
  const analyzer = new NovelAnalyzer(llm)
  let analyses: ChunkAnalysis[] = []
  let globalCharacters: Character[] = []

  if (await askContinue("æ­¥éª¤ 4/7: LLM æ·±åº¦åˆ†æ", "ğŸ” èåˆï¼šäº‹ä»¶æå– + åŠ¨æ€è§’è‰²å‘ç° + HAR")) {
    logger.info("   â³ å¼€å§‹ LLM èåˆåˆ†æï¼ˆæ­£åœ¨æå–äº‹ä»¶å¹¶åŠ¨æ€å‘ç°è§’è‰²ï¼‰...")
    const result = await analyzer.run(chunksToAnalyze, {
      limit: maxChunks,
      onProgress: (stage, curr, tot) => {
        process.stdout.write(`\r   [${stage}] ${curr}/${tot} `)
        if (curr === tot) console.log()
      },
    })
    analyses = result.analyses
    globalCharacters = result.globalCharacters

    // ä¿å­˜ä¸­é—´ç»“æœ
    await writeFile(
      join(OUTPUT_DIR, "step_4_analyses.json"),
      JSON.stringify({ analyses, globalCharacters }, null, 2)
    )
    logger.info(`   âœ… åˆ†æå®Œæˆã€‚ç»“æœå·²ä¿å­˜è‡³ step_4_analyses.json`)
  } else {
    process.exit(0)
  }

  // ===== æ­¥éª¤ 5: åˆå¹¶ç»“æœ (CPC) =====
  let merged: MergedPlotGraph = {
    fullSummary: "",
    allEvents: [],
    allCharacters: new Map(),
    allRelations: [],
  }
  if (await askContinue("æ­¥éª¤ 5/7: åˆå¹¶åˆ†æç»“æœ", "ğŸ”— åŒ…å« LLM è¯­ä¹‰å»é‡ä¸å…¨å±€æ‘˜è¦ç”Ÿæˆ")) {
    const rawMerged = mergeAnalyses(analyses, globalCharacters)
    const { globalSummary, eventIdMap } = await analyzer.consolidatePlotGraph({
      fullSummary: rawMerged.fullSummary,
      allEvents: rawMerged.allEvents,
      characters: Array.from(rawMerged.allCharacters.values()),
    })

    const deduplicatedEvents = rawMerged.allEvents.filter((e) => {
      const canonical = eventIdMap.get(e.id)
      return !canonical || canonical === e.id
    })

    const refinedRelations = rawMerged.allRelations.map((rel) => ({
      ...rel,
      fromEventId: eventIdMap.get(rel.fromEventId) || rel.fromEventId,
      toEventId: eventIdMap.get(rel.toEventId) || rel.toEventId,
    }))

    merged = {
      fullSummary: globalSummary,
      allEvents: deduplicatedEvents,
      allCharacters: rawMerged.allCharacters,
      allRelations: refinedRelations,
    }
    await writeFile(join(OUTPUT_DIR, "step_5_merged.json"), JSON.stringify(merged, null, 2))
    logger.info(`   âœ… æ™ºèƒ½åˆå¹¶å®Œæˆã€‚ç»“æœå·²ä¿å­˜è‡³ step_5_merged.json`)
  }

  // ===== æ­¥éª¤ 6: è§’è‰²è§†è§‰è®¾è®¡ =====
  let visualBible: Character[] = []
  const baseCharacters = Array.from(merged.allCharacters.values()) as Character[]
  if (await askContinue("æ­¥éª¤ 6/7: è§’è‰²è§†è§‰è®¾è®¡")) {
    const designer = new CharacterDesigner(llm)
    visualBible = await designer.run(baseCharacters)
    await writeFile(
      join(OUTPUT_DIR, "step_6_visual_bible.json"),
      JSON.stringify(visualBible, null, 2)
    )
    logger.info(`   âœ… è§†è§‰è®¾è®¡å®Œæˆã€‚ç»“æœå·²ä¿å­˜è‡³ step_6_visual_bible.json`)
  } else {
    visualBible = baseCharacters
  }

  // ===== æ­¥éª¤ 7: ç”Ÿæˆç”Ÿäº§çº§å‰§æœ¬ =====
  let episodes: Episode[] = []
  if (await askContinue("æ­¥éª¤ 7/7: ç”Ÿæˆæ¼«å‰§å‰§æœ¬ (å«åˆ†é•œ)", "ğŸ“ å·²èåˆåˆ†é•œç”Ÿæˆåˆ°å‰§æœ¬åˆ›ä½œç¯èŠ‚")) {
    const rewriter = new NovelRewriter(llm)
    episodes = await rewriter.run({
      totalSummary: merged.fullSummary,
      events: merged.allEvents,
      characters: visualBible,
      relations: merged.allRelations,
    })

    // é¢„è§ˆ
    console.log(`\nğŸ“Š æœ€ç»ˆå‰§æœ¬é¢„è§ˆ:`)
    for (const ep of episodes) {
      console.log(`   ğŸ“º [ç¬¬ ${ep.number} é›†] ${ep.title}`)
      for (const s of ep.scenes) {
        console.log(`       ğŸ¬ ${s.title} (${s.shots.length} ä¸ªåˆ†é•œ)`)
      }
    }

    // ä¿å­˜
    const productionBundle = {
      metadata: {
        generatedAt: new Date().toISOString(),
        totalEpisodes: episodes.length,
        totalScenes: episodes.reduce((acc, ep) => acc + ep.scenes.length, 0),
        totalShots: episodes.reduce(
          (acc, ep) => acc + ep.scenes.reduce((sacc, s) => sacc + s.shots.length, 0),
          0
        ),
      },
      visualBible,
      episodes,
    }
    await writeFile(
      join(OUTPUT_DIR, "production_bundle.json"),
      JSON.stringify(productionBundle, null, 2)
    )
    logger.info(`\nğŸ‰ å¤„ç†æµç¨‹å…¨éƒ¨å®Œæˆï¼è¾“å‡ºç›®å½•: ${OUTPUT_DIR}`)
  }

  chunker.dispose()
  rl.close()
}

main().catch((err) => {
  logger.error("Fatal", err)
  process.exit(1)
})
