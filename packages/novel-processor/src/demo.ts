/**
 * å°è¯´å¤„ç†å™¨ Demo
 *
 * ç”¨å‡¡äººä¿®ä»™ä¼ ç¬¬ä¸€å·æµ‹è¯•å®Œæ•´æµç¨‹
 *
 * ä½¿ç”¨æ–¹æ³•ï¼š
 * 1. è®¾ç½®ç¯å¢ƒå˜é‡ ARK_API_KEYï¼ˆç«å±±å¼•æ“ï¼‰æˆ– OPENAI_API_KEY
 * 2. è¿è¡Œï¼šbun run src/demo.ts
 *
 * å¯é€‰å‚æ•°ï¼š
 *   --chunks-only    åªåˆ‡å—ï¼Œä¸è°ƒç”¨ LLM
 *   --max-chunks=N   æœ€å¤šåˆ†æ N ä¸ªå—ï¼ˆé»˜è®¤ 2ï¼‰
 */

import "dotenv/config"
import { existsSync } from "node:fs"
import { mkdir, readFile, writeFile } from "node:fs/promises"
import { join } from "node:path"
import { mergeAnalyses, NovelAnalyzer } from "./analyzer"
import { TokenChunker } from "./chunker"
import { LLMClient } from "./llm"
import { NovelRewriter } from "./rewriter"
import type { ChunkAnalysis } from "./types"

// ä½¿ç”¨ç¬¬ä¸€å·
const NOVEL_PATH = join(__dirname, "../assets/å‡¡äººä¿®ä»™ä¼ _ç¬¬ä¸€å·_ä¸Š.txt")
const OUTPUT_DIR = join(__dirname, "../output")

// è§£æå‘½ä»¤è¡Œå‚æ•°
const args = process.argv.slice(2)
const chunksOnly = args.includes("--chunks-only")
const maxChunksArg = args.find((a) => a.startsWith("--max-chunks="))
const maxChunks = maxChunksArg ? parseInt(maxChunksArg.split("=")[1], 10) : 3

/**
 * è·å– chunk åˆ†æç»“æœçš„ç¼“å­˜è·¯å¾„
 */
function getChunkCachePath(chunkIndex: number): string {
  return join(OUTPUT_DIR, `chunk_${chunkIndex}_analysis.json`)
}

/**
 * åŠ è½½å·²ç¼“å­˜çš„ chunk åˆ†æç»“æœ
 */
async function loadCachedAnalysis(chunkIndex: number): Promise<ChunkAnalysis | null> {
  const cachePath = getChunkCachePath(chunkIndex)
  if (!existsSync(cachePath)) {
    return null
  }
  const content = await readFile(cachePath, "utf-8")
  return JSON.parse(content) as ChunkAnalysis
}

/**
 * ä¿å­˜ chunk åˆ†æç»“æœåˆ°ç¼“å­˜
 */
async function saveCachedAnalysis(chunkIndex: number, analysis: ChunkAnalysis): Promise<void> {
  const cachePath = getChunkCachePath(chunkIndex)
  await writeFile(cachePath, JSON.stringify(analysis, null, 2))
}

async function main() {
  console.log("ğŸ“š å°è¯´å¤„ç†å™¨ Demo - å‡¡äººä¿®ä»™ä¼ ç¬¬ä¸€å·\n")

  // 1. è¯»å–å°è¯´
  console.log("1ï¸âƒ£ è¯»å–å°è¯´æ–‡ä»¶...")
  const novelText = await readFile(NOVEL_PATH, "utf-8")
  console.log(`   æ–‡ä»¶å¤§å°: ${(novelText.length / 1024).toFixed(2)} KB`)
  console.log(`   å­—ç¬¦æ•°: ${novelText.length.toLocaleString()}`)

  // 2. ç»Ÿè®¡ä¿¡æ¯
  console.log("\n2ï¸âƒ£ åˆ†ææ–‡æœ¬ç»Ÿè®¡...")
  const chunker = new TokenChunker(20000, 500) // 50K tokens per chunkï¼Œé¿å… Lost in the Middle
  const stats = chunker.getStats(novelText)
  console.log(`   æ€» token æ•°: ${stats.totalTokens.toLocaleString()}`)
  console.log(`   é¢„è®¡åˆ‡å—æ•°: ${stats.estimatedChunks}`)
  console.log(`   é¢„ä¼°è´¹ç”¨: Â¥${((stats.totalTokens / 1000000) * 1.2).toFixed(2)} (è¾“å…¥) + è¾“å‡ºè´¹ç”¨`)

  // 3. åˆ‡å—
  console.log("\n3ï¸âƒ£ åˆ‡åˆ†æ–‡æœ¬å—...")
  const chunks = chunker.chunk(novelText)
  console.log(`   å®é™…åˆ‡åˆ†æˆ ${chunks.length} å—`)

  for (const chunk of chunks) {
    console.log(
      `   - å— ${chunk.index}: ${chunk.tokenCount.toLocaleString()} tokens, ${chunk.content.length.toLocaleString()} chars`
    )
  }

  // ä¿å­˜åˆ‡å—ç»“æœ
  await mkdir(OUTPUT_DIR, { recursive: true })
  for (const chunk of chunks) {
    await writeFile(join(OUTPUT_DIR, `chunk_${chunk.index}.txt`), chunk.content)
  }
  console.log(`\nğŸ“ åˆ‡å—ç»“æœå·²ä¿å­˜åˆ° ${OUTPUT_DIR}/chunk_*.txt`)

  // å¦‚æœåªåˆ‡å—ï¼Œåˆ°æ­¤ç»“æŸ
  if (chunksOnly) {
    console.log("\nâœ… åˆ‡å—å®Œæˆï¼ˆ--chunks-only æ¨¡å¼ï¼‰")
    chunker.dispose()
    return
  }

  // 4. æ£€æŸ¥ API key
  if (!process.env.ARK_API_KEY && !process.env.OPENAI_API_KEY && !process.env.DEEPSEEK_API_KEY) {
    console.log("\nâš ï¸  æœªè®¾ç½® ARK_API_KEYã€OPENAI_API_KEY æˆ– DEEPSEEK_API_KEYï¼Œè·³è¿‡ LLM åˆ†ææ­¥éª¤")
    console.log("   è®¾ç½®ç¯å¢ƒå˜é‡åé‡æ–°è¿è¡Œï¼Œæˆ–ä½¿ç”¨ --chunks-only åªæŸ¥çœ‹åˆ‡å—ç»“æœ")
    chunker.dispose()
    return
  }

  // 5. LLM åˆ†æ
  const chunksToAnalyze = chunks.slice(0, maxChunks)
  const analyzeTokens = chunksToAnalyze.reduce((sum, c) => sum + c.tokenCount, 0)

  console.log(`\n4ï¸âƒ£ ä½¿ç”¨ LLM åˆ†æå†…å®¹...`)
  console.log(
    `   å°†åˆ†æ ${chunksToAnalyze.length} ä¸ªå—ï¼ˆå…± ${analyzeTokens.toLocaleString()} tokensï¼‰`
  )

  const llm = new LLMClient()
  const analyzer = new NovelAnalyzer(llm)
  let analyses: ChunkAnalysis[] = []

  // æ£€æŸ¥ç¼“å­˜
  const cachedAnalyses = await Promise.all(chunksToAnalyze.map((c) => loadCachedAnalysis(c.index)))
  const allCached = cachedAnalyses.every((a) => a !== null)

  if (allCached) {
    console.log("   æ‰€æœ‰å—å‡å·²ç¼“å­˜ï¼Œç›´æ¥åŠ è½½...")
    analyses = cachedAnalyses as ChunkAnalysis[]
  } else {
    // è°ƒç”¨å¹¶è¡Œåˆ†æç®¡é“
    // ç­–ç•¥æµ‹è¯•ï¼šä»…å¯¹å‰ maxChunks ä¸ªå—æ‰§è¡Œ Discovery å’Œ Extraction
    analyses = await analyzer.analyzeChunks(chunksToAnalyze, {
      limit: maxChunks,
      concurrency: 5,
      onProgress: (stage, current, total) => {
        const percent = Math.round((current / total) * 100)
        process.stdout.write(`\r   [${stage}] è¿›åº¦: ${percent}% (${current}/${total})        `)
        if (current === total) console.log()
      },
    })

    // ä¿å­˜æ–°äº§ç”Ÿçš„åˆ†æç»“æœåˆ°ç¼“å­˜
    for (let i = 0; i < analyses.length; i++) {
      if (!cachedAnalyses[i]) {
        await saveCachedAnalysis(chunksToAnalyze[i].index, analyses[i])
      }
    }
    console.log("   åˆ†æå®Œæˆå¹¶å·²æ›´æ–°ç¼“å­˜")
  }

  // 6. åˆå¹¶ç»“æœ
  console.log("\n5ï¸âƒ£ åˆå¹¶åˆ†æç»“æœ (Causal Plot-graph Construction)...")
  const merged = mergeAnalyses(analyses)

  // 7. Rewriter é˜¶æ®µ - ç”Ÿæˆå‰§æœ¬ (R2 è®ºæ–‡æ ¸å¿ƒé€»è¾‘)
  console.log("\n6ï¸âƒ£ ä½¿ç”¨ Rewriter ç”Ÿæˆæ¼«å‰§å‰§æœ¬ (R2 Pipeline)...")
  const rewriter = new NovelRewriter(llm)

  // A. é¦–å…ˆæ ¹æ®åˆ†æç»“æœç”Ÿæˆå‰§æœ¬å¤§çº²å’Œåœºæ™¯è®¡åˆ’
  console.log("   æ­£åœ¨è§„åˆ’å‰§æœ¬å¤§çº²ä¸åœºæ¬¡...")
  const finalAnalysisData = {
    totalSummary: merged.fullSummary,
    events: merged.allEvents,
    characters: Array.from(merged.allCharacters.values()),
    relations: merged.allRelations,
  }

  const episode = await rewriter.convertToEpisode(finalAnalysisData, chunks)
  console.log(`\nâœ… å‰§æœ¬ç”Ÿæˆå®Œæˆï¼åŒ…å« ${episode.scenes.length} ä¸ªåœºæ™¯`)

  // 8. ä¿å­˜æœ€ç»ˆç»“æœ
  const timestamp = new Date().toISOString().replace(/[:.]/g, "-").slice(0, 19)
  const resultFilename = `episode_result_${timestamp}.json`
  await writeFile(join(OUTPUT_DIR, resultFilename), JSON.stringify(episode, null, 2))

  console.log(`\nğŸ“Š å‰§æœ¬å†…å®¹é¢„è§ˆ:`)
  console.log(`   ä¸»æ—¨: ${episode.title}`)
  for (const scene of episode.scenes) {
    console.log(`   ğŸ¬ [åœºæ™¯] ${scene.title}`)
    console.log(`       åœ°ç‚¹: ${scene.setting}`)
    console.log(`       è§†è§‰å»ºè®®: ${scene.visualDescription.slice(0, 50)}...`)
  }

  console.log(`\nğŸ“ å®Œæ•´å‰§æœ¬å·²ä¿å­˜åˆ° ${OUTPUT_DIR}/${resultFilename}`)

  chunker.dispose()
}

main().catch(console.error)
